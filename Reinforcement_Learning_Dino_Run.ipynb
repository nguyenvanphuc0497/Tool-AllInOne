{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nguyenvanphuc0497/Tool-AllInOne/blob/develop/Reinforcement_Learning_Dino_Run.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "VJfXFRj7eyi9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e7b47b6-8e2e-450e-eaac-5f0d7cceb8a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: opencv-python in ./.venv/lib/python3.9/site-packages (4.8.0.76)\n",
            "Requirement already satisfied: numpy>=1.19.3 in ./.venv/lib/python3.9/site-packages (from opencv-python) (1.25.2)\n",
            "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 23.2.1 is available.\n",
            "You should consider upgrading via the '/Users/PhucMacbookPro/OtherProject/Tool-AllInOne/.venv/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: keras in ./.venv/lib/python3.9/site-packages (2.13.1)\n",
            "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 23.2.1 is available.\n",
            "You should consider upgrading via the '/Users/PhucMacbookPro/OtherProject/Tool-AllInOne/.venv/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Collecting tensorflow\n",
            "  Using cached tensorflow-2.13.0-cp39-cp39-macosx_10_15_x86_64.whl (216.2 MB)\n",
            "Requirement already satisfied: packaging in ./.venv/lib/python3.9/site-packages (from tensorflow) (23.1)\n",
            "Requirement already satisfied: six>=1.12.0 in ./.venv/lib/python3.9/site-packages (from tensorflow) (1.16.0)\n",
            "Collecting termcolor>=1.1.0\n",
            "  Downloading termcolor-2.3.0-py3-none-any.whl (6.9 kB)\n",
            "Collecting astunparse>=1.6.0\n",
            "  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
            "Collecting gast<=0.4.0,>=0.2.1\n",
            "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
            "Collecting tensorboard<2.14,>=2.13\n",
            "  Using cached tensorboard-2.13.0-py3-none-any.whl (5.6 MB)\n",
            "Collecting google-pasta>=0.1.1\n",
            "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
            "Collecting absl-py>=1.0.0\n",
            "  Using cached absl_py-1.4.0-py3-none-any.whl (126 kB)\n",
            "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3\n",
            "  Using cached protobuf-4.24.2-cp37-abi3-macosx_10_9_universal2.whl (409 kB)\n",
            "Collecting tensorflow-estimator<2.14,>=2.13.0\n",
            "  Using cached tensorflow_estimator-2.13.0-py2.py3-none-any.whl (440 kB)\n",
            "Requirement already satisfied: setuptools in ./.venv/lib/python3.9/site-packages (from tensorflow) (58.0.4)\n",
            "Collecting opt-einsum>=2.3.2\n",
            "  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
            "Requirement already satisfied: keras<2.14,>=2.13.1 in ./.venv/lib/python3.9/site-packages (from tensorflow) (2.13.1)\n",
            "Collecting h5py>=2.9.0\n",
            "  Downloading h5py-3.9.0-cp39-cp39-macosx_10_9_x86_64.whl (3.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2 MB 1.1 MB/s \n",
            "\u001b[?25hCollecting typing-extensions<4.6.0,>=3.6.6\n",
            "  Downloading typing_extensions-4.5.0-py3-none-any.whl (27 kB)\n",
            "Collecting grpcio<2.0,>=1.24.3\n",
            "  Using cached grpcio-1.57.0-cp39-cp39-macosx_10_10_universal2.whl (9.1 MB)\n",
            "Collecting wrapt>=1.11.0\n",
            "  Downloading wrapt-1.15.0-cp39-cp39-macosx_10_9_x86_64.whl (35 kB)\n",
            "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
            "  Downloading tensorflow_io_gcs_filesystem-0.33.0-cp39-cp39-macosx_10_14_x86_64.whl (1.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.7 MB 1.0 MB/s \n",
            "\u001b[?25hCollecting flatbuffers>=23.1.21\n",
            "  Using cached flatbuffers-23.5.26-py2.py3-none-any.whl (26 kB)\n",
            "Collecting libclang>=13.0.0\n",
            "  Using cached libclang-16.0.6-py2.py3-none-macosx_10_9_x86_64.whl (24.5 MB)\n",
            "Collecting numpy<=1.24.3,>=1.22\n",
            "  Downloading numpy-1.24.3-cp39-cp39-macosx_10_9_x86_64.whl (19.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 19.8 MB 1.7 MB/s \n",
            "\u001b[?25hCollecting wheel<1.0,>=0.23.0\n",
            "  Downloading wheel-0.41.2-py3-none-any.whl (64 kB)\n",
            "\u001b[K     |████████████████████████████████| 64 kB 1.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.21.0 in ./.venv/lib/python3.9/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (2.31.0)\n",
            "Collecting google-auth-oauthlib<1.1,>=0.5\n",
            "  Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
            "Collecting werkzeug>=1.0.1\n",
            "  Downloading werkzeug-2.3.7-py3-none-any.whl (242 kB)\n",
            "\u001b[K     |████████████████████████████████| 242 kB 866 kB/s \n",
            "\u001b[?25hCollecting tensorboard-data-server<0.8.0,>=0.7.0\n",
            "  Downloading tensorboard_data_server-0.7.1-py3-none-macosx_10_9_x86_64.whl (4.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.8 MB 870 kB/s \n",
            "\u001b[?25hCollecting markdown>=2.6.8\n",
            "  Downloading Markdown-3.4.4-py3-none-any.whl (94 kB)\n",
            "\u001b[K     |████████████████████████████████| 94 kB 844 kB/s \n",
            "\u001b[?25hCollecting google-auth<3,>=1.6.3\n",
            "  Downloading google_auth-2.22.0-py2.py3-none-any.whl (181 kB)\n",
            "\u001b[K     |████████████████████████████████| 181 kB 891 kB/s \n",
            "\u001b[?25hCollecting pyasn1-modules>=0.2.1\n",
            "  Downloading pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n",
            "\u001b[K     |████████████████████████████████| 181 kB 930 kB/s \n",
            "\u001b[?25hCollecting urllib3<2.0\n",
            "  Downloading urllib3-1.26.16-py2.py3-none-any.whl (143 kB)\n",
            "\u001b[K     |████████████████████████████████| 143 kB 912 kB/s \n",
            "\u001b[?25hCollecting cachetools<6.0,>=2.0.0\n",
            "  Downloading cachetools-5.3.1-py3-none-any.whl (9.3 kB)\n",
            "Collecting rsa<5,>=3.1.4\n",
            "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
            "Collecting requests-oauthlib>=0.7.0\n",
            "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in ./.venv/lib/python3.9/site-packages (from markdown>=2.6.8->tensorboard<2.14,>=2.13->tensorflow) (6.8.0)\n",
            "Requirement already satisfied: zipp>=0.5 in ./.venv/lib/python3.9/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.14,>=2.13->tensorflow) (3.16.2)\n",
            "Collecting pyasn1<0.6.0,>=0.4.6\n",
            "  Downloading pyasn1-0.5.0-py2.py3-none-any.whl (83 kB)\n",
            "\u001b[K     |████████████████████████████████| 83 kB 990 kB/s \n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (2023.7.22)\n",
            "Collecting oauthlib>=3.0.0\n",
            "  Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
            "\u001b[K     |████████████████████████████████| 151 kB 923 kB/s \n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.1.1 in ./.venv/lib/python3.9/site-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow) (2.1.3)\n",
            "Installing collected packages: urllib3, pyasn1, rsa, pyasn1-modules, oauthlib, cachetools, requests-oauthlib, google-auth, wheel, werkzeug, tensorboard-data-server, protobuf, numpy, markdown, grpcio, google-auth-oauthlib, absl-py, wrapt, typing-extensions, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard, opt-einsum, libclang, h5py, google-pasta, gast, flatbuffers, astunparse, tensorflow\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.0.4\n",
            "    Uninstalling urllib3-2.0.4:\n",
            "      Successfully uninstalled urllib3-2.0.4\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.25.2\n",
            "    Uninstalling numpy-1.25.2:\n",
            "      Successfully uninstalled numpy-1.25.2\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing-extensions 4.7.1\n",
            "    Uninstalling typing-extensions-4.7.1:\n",
            "      Successfully uninstalled typing-extensions-4.7.1\n",
            "Successfully installed absl-py-1.4.0 astunparse-1.6.3 cachetools-5.3.1 flatbuffers-23.5.26 gast-0.4.0 google-auth-2.22.0 google-auth-oauthlib-1.0.0 google-pasta-0.2.0 grpcio-1.57.0 h5py-3.9.0 libclang-16.0.6 markdown-3.4.4 numpy-1.24.3 oauthlib-3.2.2 opt-einsum-3.3.0 protobuf-4.24.2 pyasn1-0.5.0 pyasn1-modules-0.3.0 requests-oauthlib-1.3.1 rsa-4.9 tensorboard-2.13.0 tensorboard-data-server-0.7.1 tensorflow-2.13.0 tensorflow-estimator-2.13.0 tensorflow-io-gcs-filesystem-0.33.0 termcolor-2.3.0 typing-extensions-4.5.0 urllib3-1.26.16 werkzeug-2.3.7 wheel-0.41.2 wrapt-1.15.0\n",
            "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 23.2.1 is available.\n",
            "You should consider upgrading via the '/Users/PhucMacbookPro/OtherProject/Tool-AllInOne/.venv/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: selenium in ./.venv/lib/python3.9/site-packages (4.12.0)\n",
            "Requirement already satisfied: certifi>=2021.10.8 in ./.venv/lib/python3.9/site-packages (from selenium) (2023.7.22)\n",
            "Requirement already satisfied: trio~=0.17 in ./.venv/lib/python3.9/site-packages (from selenium) (0.22.2)\n",
            "Requirement already satisfied: trio-websocket~=0.9 in ./.venv/lib/python3.9/site-packages (from selenium) (0.10.3)\n",
            "Requirement already satisfied: urllib3[socks]<3,>=1.26 in ./.venv/lib/python3.9/site-packages (from selenium) (1.26.16)\n",
            "Requirement already satisfied: outcome in ./.venv/lib/python3.9/site-packages (from trio~=0.17->selenium) (1.2.0)\n",
            "Requirement already satisfied: attrs>=20.1.0 in ./.venv/lib/python3.9/site-packages (from trio~=0.17->selenium) (23.1.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.0rc9 in ./.venv/lib/python3.9/site-packages (from trio~=0.17->selenium) (1.1.3)\n",
            "Requirement already satisfied: sniffio in ./.venv/lib/python3.9/site-packages (from trio~=0.17->selenium) (1.3.0)\n",
            "Requirement already satisfied: idna in ./.venv/lib/python3.9/site-packages (from trio~=0.17->selenium) (3.4)\n",
            "Requirement already satisfied: sortedcontainers in ./.venv/lib/python3.9/site-packages (from trio~=0.17->selenium) (2.4.0)\n",
            "Requirement already satisfied: wsproto>=0.14 in ./.venv/lib/python3.9/site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
            "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in ./.venv/lib/python3.9/site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in ./.venv/lib/python3.9/site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
            "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 23.2.1 is available.\n",
            "You should consider upgrading via the '/Users/PhucMacbookPro/OtherProject/Tool-AllInOne/.venv/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install opencv-python\n",
        "%pip install keras\n",
        "%pip install tensorflow\n",
        "%pip install selenium\n",
        "%pip install Pillow\n",
        "%pip install pandas\n",
        "%pip install webdriver-manager\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "1Af9y7I0eyjB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "486d532e-9665-408d-be88-6cc55968dbb0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-09-05 11:33:22.331743: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[11], line 22\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m model_from_json\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sequential\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dense, Dropout, Activation, Flatten\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconvolutional\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Conv2D, MaxPooling2D\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptimizers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SGD , Adam\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras.layers.core'"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from PIL import Image\n",
        "import cv2 #opencv\n",
        "import io\n",
        "import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from IPython.display import clear_output\n",
        "from random import randint\n",
        "import os\n",
        "\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from webdriver_manager.chrome import ChromeDriverManager\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.keys import Keys\n",
        "\n",
        "#keras imports\n",
        "from keras.models import model_from_json\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
        "from keras.optimizers import SGD , Adam\n",
        "from keras.callbacks import TensorBoard\n",
        "from collections import deque\n",
        "import random\n",
        "import pickle\n",
        "from io import BytesIO\n",
        "import base64\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "3iLVidAoeyjB"
      },
      "outputs": [],
      "source": [
        "#path variables\n",
        "game_url = \"chrome://dino\"\n",
        "chrome_driver_path = \"../chromedriver\"\n",
        "loss_file_path = \"./objects/loss_df.csv\"\n",
        "actions_file_path = \"./objects/actions_df.csv\"\n",
        "q_value_file_path = \"./objects/q_values.csv\"\n",
        "scores_file_path = \"./objects/scores_df.csv\"\n",
        "\n",
        "#scripts\n",
        "#create id for canvas for faster selection from DOM\n",
        "init_script = \"document.getElementsByClassName('runner-canvas')[0].id = 'runner-canvas'\"\n",
        "\n",
        "#get image from canvas\n",
        "getbase64Script = \"canvasRunner = document.getElementById('runner-canvas'); \\\n",
        "return canvasRunner.toDataURL().substring(22)\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "5SDuhM6BeyjC"
      },
      "outputs": [],
      "source": [
        "\n",
        "# from selenium.webdriver.chrome.service import Service\n",
        "# from webdriver_manager.chrome import ChromeDriverManager\n",
        "# from selenium.webdriver.common.by import By\n",
        "# import time\n",
        "\n",
        "'''\n",
        "* Game class: Selenium interfacing between the python and browser\n",
        "* __init__():  Launch the broswer window using the attributes in chrome_options\n",
        "* get_crashed() : return true if the agent as crashed on an obstacles. Gets javascript variable from game decribing the state\n",
        "* get_playing(): true if game in progress, false is crashed or paused\n",
        "* restart() : sends a signal to browser-javascript to restart the game\n",
        "* press_up(): sends a single to press up get to the browser\n",
        "* get_score(): gets current game score from javascript variables.\n",
        "* pause(): pause the game\n",
        "* resume(): resume a paused game if not crashed\n",
        "* end(): close the browser and end the game\n",
        "'''\n",
        "class Game:\n",
        "    def __init__(self,custom_config=True):\n",
        "        chrome_options = Options()\n",
        "        chrome_options.add_argument(\"disable-infobars\")\n",
        "        chrome_options.add_argument(\"--mute-audio\")\n",
        "        # chrome_options.add_argument(\"start-maximized\")\n",
        "        # self._driver = webdriver.Chrome(executable_path = chrome_driver_path,chrome_options=chrome_options)\n",
        "        self._driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
        "        self._driver.set_window_position(x=-10,y=0)\n",
        "        try:\n",
        "            self._driver.get('chrome://dino')\n",
        "        except:\n",
        "            print(\"continue with \")\n",
        "        self._driver.execute_script(\"Runner.config.ACCELERATION=0\")\n",
        "        self._driver.execute_script(init_script)\n",
        "    def get_crashed(self):\n",
        "        return self._driver.execute_script(\"return Runner.instance_.crashed\")\n",
        "    def get_playing(self):\n",
        "        return self._driver.execute_script(\"return Runner.instance_.playing\")\n",
        "    def restart(self):\n",
        "        self._driver.execute_script(\"Runner.instance_.restart()\")\n",
        "    def press_up(self):\n",
        "        # self._driver.find_element_by_tag_name(\"body\").send_keys(Keys.ARROW_UP)\n",
        "        self._driver.find_element(By.TAG_NAME,\"body\").send_keys(Keys.ARROW_UP)\n",
        "    def get_score(self):\n",
        "        score_array = self._driver.execute_script(\"return Runner.instance_.distanceMeter.digits\")\n",
        "        score = ''.join(score_array) # the javascript object is of type array with score in the formate[1,0,0] which is 100.\n",
        "        return int(score)\n",
        "    def pause(self):\n",
        "        return self._driver.execute_script(\"return Runner.instance_.stop()\")\n",
        "    def resume(self):\n",
        "        return self._driver.execute_script(\"return Runner.instance_.play()\")\n",
        "    def end(self):\n",
        "        self._driver.close()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "0l1YxSGseyjD"
      },
      "outputs": [],
      "source": [
        "class DinoAgent:\n",
        "    def __init__(self,game): #takes game as input for taking actions\n",
        "        self._game = game;\n",
        "        self.jump(); #to start the game, we need to jump once\n",
        "    def is_running(self):\n",
        "        return self._game.get_playing()\n",
        "    def is_crashed(self):\n",
        "        return self._game.get_crashed()\n",
        "    def jump(self):\n",
        "        self._game.press_up()\n",
        "    def duck(self):\n",
        "        self._game.press_down()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "W5gUedHPeyjD"
      },
      "outputs": [],
      "source": [
        "class Game_sate:\n",
        "    def __init__(self,agent,game):\n",
        "        self._agent = agent\n",
        "        self._game = game\n",
        "        self._display = show_img() #display the processed image on screen using openCV, implemented using python coroutine\n",
        "        self._display.__next__() # initiliaze the display coroutine\n",
        "    def get_state(self,actions):\n",
        "        actions_df.loc[len(actions_df)] = actions[1] # storing actions in a dataframe\n",
        "        score = self._game.get_score()\n",
        "        reward = 0.1\n",
        "        is_over = False #game over\n",
        "        if actions[1] == 1:\n",
        "            self._agent.jump()\n",
        "        image = grab_screen(self._game._driver)\n",
        "        self._display.send(image) #display the image on screen\n",
        "        if self._agent.is_crashed():\n",
        "            scores_df.loc[len(loss_df)] = score # log the score when game is over\n",
        "            self._game.restart()\n",
        "            reward = -1\n",
        "            is_over = True\n",
        "        return image, reward, is_over #return the Experience tuple"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gRfYGzNweyjE",
        "outputId": "e7d6fea2-612d-4ca5-e0b8-d4d33a750819"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "continue with \n"
          ]
        }
      ],
      "source": [
        "game = Game()\n",
        "dino = DinoAgent(game)\n",
        "game_state = Game_sate(dino,game)\n",
        "\n",
        "# game_state.get_state([ACTIONS])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "HaOnyGp8eyjE"
      },
      "outputs": [],
      "source": [
        "def save_obj(obj, name ):\n",
        "    with open('objects/'+ name + '.pkl', 'wb') as f: #dump files into objects folder\n",
        "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
        "def load_obj(name ):\n",
        "    with open('objects/' + name + '.pkl', 'rb') as f:\n",
        "        return pickle.load(f)\n",
        "\n",
        "def grab_screen(_driver):\n",
        "    image_b64 = _driver.execute_script(getbase64Script)\n",
        "    screen = np.array(Image.open(BytesIO(base64.b64decode(image_b64))))\n",
        "    image = process_img(screen)#processing image as required\n",
        "    return image\n",
        "\n",
        "def process_img(image):\n",
        "\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) #RGB to Grey Scale\n",
        "    image = image[:300, :500] #Crop Region of Interest(ROI)\n",
        "    image = cv2.resize(image, (80,80))\n",
        "    return  image\n",
        "\n",
        "def show_img(graphs = False):\n",
        "    \"\"\"\n",
        "    Show images in new window\n",
        "    \"\"\"\n",
        "    while True:\n",
        "        screen = (yield)\n",
        "        window_title = \"logs\" if graphs else \"game_play\"\n",
        "        cv2.namedWindow(window_title, cv2.WINDOW_NORMAL)\n",
        "        imS = cv2.resize(screen, (800, 400))\n",
        "        cv2.imshow(window_title, screen)\n",
        "        if (cv2.waitKey(1) & 0xFF == ord('q')):\n",
        "            cv2.destroyAllWindows()\n",
        "            break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Y47SnUeheyjF"
      },
      "outputs": [],
      "source": [
        "#Intialize log structures from file if exists else create new\n",
        "loss_df = pd.read_csv(loss_file_path) if os.path.isfile(loss_file_path) else pd.DataFrame(columns =['loss'])\n",
        "scores_df = pd.read_csv(scores_file_path) if os.path.isfile(loss_file_path) else pd.DataFrame(columns = ['scores'])\n",
        "actions_df = pd.read_csv(actions_file_path) if os.path.isfile(actions_file_path) else pd.DataFrame(columns = ['actions'])\n",
        "q_values_df =pd.read_csv(actions_file_path) if os.path.isfile(q_value_file_path) else pd.DataFrame(columns = ['qvalues'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "R-FxWzYXeyjF"
      },
      "outputs": [],
      "source": [
        "#game parameters\n",
        "ACTIONS = 2 # possible actions: jump, do nothing\n",
        "GAMMA = 0.99 # decay rate of past observations original 0.99\n",
        "OBSERVATION = 100. # timesteps to observe before training\n",
        "EXPLORE = 100000  # frames over which to anneal epsilon\n",
        "FINAL_EPSILON = 0.0001 # final value of epsilon\n",
        "INITIAL_EPSILON = 0.1 # starting value of epsilon\n",
        "REPLAY_MEMORY = 50000 # number of previous transitions to remember\n",
        "BATCH = 16 # size of minibatch\n",
        "FRAME_PER_ACTION = 1\n",
        "LEARNING_RATE = 1e-4\n",
        "img_rows , img_cols = 80,80\n",
        "img_channels = 4 #We stack 4 frames"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "SuIhI_MFeyjF"
      },
      "outputs": [],
      "source": [
        "# training variables saved as checkpoints to filesystem to resume training from the same step\n",
        "def init_cache():\n",
        "    \"\"\"initial variable caching, done only once\"\"\"\n",
        "    save_obj(INITIAL_EPSILON,\"epsilon\")\n",
        "    t = 0\n",
        "    save_obj(t,\"time\")\n",
        "    D = deque()\n",
        "    save_obj(D,\"D\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0__jx4soeyjG",
        "outputId": "33593338-6651-44a5-c4f2-60e05a8ef542"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Call only once to init file structure\\n'"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''Call only once to init file structure\n",
        "'''\n",
        "# init_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EdZqqnFReyjG"
      },
      "outputs": [],
      "source": [
        "def buildmodel():\n",
        "    print(\"Now we build the model\")\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(32, (8, 8), padding='same',strides=(4, 4),input_shape=(img_cols,img_rows,img_channels)))  #80*80*4\n",
        "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Conv2D(64, (4, 4),strides=(2, 2),  padding='same'))\n",
        "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Conv2D(64, (3, 3),strides=(1, 1),  padding='same'))\n",
        "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(512))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dense(ACTIONS))\n",
        "    adam = Adam(learning_rate=LEARNING_RATE)\n",
        "    model.compile(loss='mse',optimizer=adam)\n",
        "\n",
        "    #create model file if not present\n",
        "    if not os.path.isfile(loss_file_path):\n",
        "        model.save_weights('model.h5')\n",
        "    print(\"We finish building the model\")\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "H_OPlqg3eyjG"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "main training module\n",
        "Parameters:\n",
        "* model => Keras Model to be trained\n",
        "* game_state => Game State module with access to game environment and dino\n",
        "* observe => flag to indicate wherther the model is to be trained(weight updates), else just play\n",
        "'''\n",
        "def trainNetwork(model,game_state,observe=False):\n",
        "    last_time = time.time()\n",
        "    # store the previous observations in replay memory\n",
        "    D = load_obj(\"D\") #load from file system\n",
        "    # get the first state by doing nothing\n",
        "    do_nothing = np.zeros(ACTIONS)\n",
        "    do_nothing[0] =1 #0 => do nothing,\n",
        "                     #1=> jump\n",
        "\n",
        "    x_t, r_0, terminal = game_state.get_state(do_nothing) # get next step after performing the action\n",
        "\n",
        "\n",
        "    s_t = np.stack((x_t, x_t, x_t, x_t), axis=2) # stack 4 images to create placeholder input\n",
        "\n",
        "\n",
        "\n",
        "    s_t = s_t.reshape(1, s_t.shape[0], s_t.shape[1], s_t.shape[2])  #1*20*40*4\n",
        "\n",
        "    initial_state = s_t\n",
        "\n",
        "    if observe :\n",
        "        OBSERVE = 999999999    #We keep observe, never train\n",
        "        epsilon = FINAL_EPSILON\n",
        "        print (\"Now we load weight\")\n",
        "        model.load_weights(\"model.h5\")\n",
        "        adam = Adam(learning_rate=LEARNING_RATE)\n",
        "        model.compile(loss='mse',optimizer=adam)\n",
        "        print (\"Weight load successfully\")\n",
        "    else:                       #We go to training mode\n",
        "        OBSERVE = OBSERVATION\n",
        "        epsilon = load_obj(\"epsilon\")\n",
        "        model.load_weights(\"model.h5\")\n",
        "        adam = Adam(learning_rate=LEARNING_RATE)\n",
        "        model.compile(loss='mse',optimizer=adam)\n",
        "\n",
        "    t = load_obj(\"time\") # resume from the previous time step stored in file system\n",
        "    while (True): #endless running\n",
        "\n",
        "        loss = 0\n",
        "        Q_sa = 0\n",
        "        action_index = 0\n",
        "        r_t = 0 #reward at 4\n",
        "        a_t = np.zeros([ACTIONS]) # action at t\n",
        "\n",
        "        #choose an action epsilon greedy\n",
        "        if t % FRAME_PER_ACTION == 0: #parameter to skip frames for actions\n",
        "            if  random.random() <= epsilon: #randomly explore an action\n",
        "                print(\"----------Random Action----------\")\n",
        "                action_index = random.randrange(ACTIONS)\n",
        "                a_t[action_index] = 1\n",
        "            else: # predict the output\n",
        "                q = model.predict(s_t)       #input a stack of 4 images, get the prediction\n",
        "                max_Q = np.argmax(q)         # chosing index with maximum q value\n",
        "                action_index = max_Q\n",
        "                a_t[action_index] = 1        # o=> do nothing, 1=> jump\n",
        "\n",
        "        #We reduced the epsilon (exploration parameter) gradually\n",
        "        if epsilon > FINAL_EPSILON and t > OBSERVE:\n",
        "            epsilon -= (INITIAL_EPSILON - FINAL_EPSILON) / EXPLORE\n",
        "\n",
        "        #run the selected action and observed next state and reward\n",
        "        x_t1, r_t, terminal = game_state.get_state(a_t)\n",
        "        print('fps: {0}'.format(1 / (time.time()-last_time))) # helpful for measuring frame rate\n",
        "        last_time = time.time()\n",
        "        x_t1 = x_t1.reshape(1, x_t1.shape[0], x_t1.shape[1], 1) #1x20x40x1\n",
        "        s_t1 = np.append(x_t1, s_t[:, :, :, :3], axis=3) # append the new image to input stack and remove the first one\n",
        "\n",
        "\n",
        "        # store the transition in D\n",
        "        D.append((s_t, action_index, r_t, s_t1, terminal))\n",
        "        if len(D) > REPLAY_MEMORY:\n",
        "            D.popleft()\n",
        "\n",
        "        #only train if done observing\n",
        "        if t > OBSERVE:\n",
        "\n",
        "            #sample a minibatch to train on\n",
        "            minibatch = random.sample(D, BATCH)\n",
        "            inputs = np.zeros((BATCH, s_t.shape[1], s_t.shape[2], s_t.shape[3]))   #32, 20, 40, 4\n",
        "            targets = np.zeros((inputs.shape[0], ACTIONS))                         #32, 2\n",
        "\n",
        "            #Now we do the experience replay\n",
        "            for i in range(0, len(minibatch)):\n",
        "                state_t = minibatch[i][0]    # 4D stack of images\n",
        "                action_t = minibatch[i][1]   #This is action index\n",
        "                reward_t = minibatch[i][2]   #reward at state_t due to action_t\n",
        "                state_t1 = minibatch[i][3]   #next state\n",
        "                terminal = minibatch[i][4]   #wheather the agent died or survided due the action\n",
        "\n",
        "\n",
        "                inputs[i:i + 1] = state_t\n",
        "\n",
        "                targets[i] = model.predict(state_t)  # predicted q values\n",
        "                Q_sa = model.predict(state_t1)      #predict q values for next step\n",
        "\n",
        "                if terminal:\n",
        "                    targets[i, action_t] = reward_t # if terminated, only equals reward\n",
        "                else:\n",
        "                    targets[i, action_t] = reward_t + GAMMA * np.max(Q_sa)\n",
        "\n",
        "            loss += model.train_on_batch(inputs, targets)\n",
        "            loss_df.loc[len(loss_df)] = loss\n",
        "            q_values_df.loc[len(q_values_df)] = np.max(Q_sa)\n",
        "        s_t = initial_state if terminal else s_t1 #reset game to initial frame if terminate\n",
        "        t = t + 1\n",
        "\n",
        "        # save progress every 1000 iterations\n",
        "        if t % 1000 == 0:\n",
        "            print(\"Now we save model\")\n",
        "            game_state._game.pause() #pause game while saving to filesystem\n",
        "            model.save_weights(\"model.h5\", overwrite=True)\n",
        "            save_obj(D,\"D\") #saving episodes\n",
        "            save_obj(t,\"time\") #caching time steps\n",
        "            save_obj(epsilon,\"epsilon\") #cache epsilon to avoid repeated randomness in actions\n",
        "            loss_df.to_csv(\"./objects/loss_df.csv\",index=False)\n",
        "            scores_df.to_csv(\"./objects/scores_df.csv\",index=False)\n",
        "            actions_df.to_csv(\"./objects/actions_df.csv\",index=False)\n",
        "            q_values_df.to_csv(q_value_file_path,index=False)\n",
        "            with open(\"model.json\", \"w\") as outfile:\n",
        "                json.dump(model.to_json(), outfile)\n",
        "            clear_output()\n",
        "            game_state._game.resume()\n",
        "        # print info\n",
        "        state = \"\"\n",
        "        if t <= OBSERVE:\n",
        "            state = \"observe\"\n",
        "        elif t > OBSERVE and t <= OBSERVE + EXPLORE:\n",
        "            state = \"explore\"\n",
        "        else:\n",
        "            state = \"train\"\n",
        "\n",
        "        print(\"TIMESTEP\", t, \"/ STATE\", state,             \"/ EPSILON\", epsilon, \"/ ACTION\", action_index, \"/ REWARD\", r_t,             \"/ Q_MAX \" , np.max(Q_sa), \"/ Loss \", loss)\n",
        "\n",
        "    print(\"Episode finished!\")\n",
        "    print(\"************************\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "eO-0IZ-FeyjG"
      },
      "outputs": [],
      "source": [
        "#main function\n",
        "def playGame(observe=False):\n",
        "    game = Game()\n",
        "    dino = DinoAgent(game)\n",
        "    game_state = Game_sate(dino,game)\n",
        "    model = buildmodel()\n",
        "    try:\n",
        "        trainNetwork(model,game_state,observe=observe)\n",
        "    except StopIteration:\n",
        "        game.end()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vCHzXPDEeyjG"
      },
      "outputs": [],
      "source": [
        "# Test Game class\n",
        "# a =Game()\n",
        "# a.press_up()\n",
        "# while a.get_crashed()!=True:\n",
        "#     time.sleep(1)\n",
        "#     print(a.get_score())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "P0weJnsLeyjG",
        "outputId": "d4c4332b-9c8b-44a1-fdbf-13e6240c059e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "continue with \n",
            "Now we build the model\n",
            "We finish building the model\n",
            "1/1 [==============================] - 0s 93ms/step\n",
            "fps: 4.1702790225510835\n",
            "TIMESTEP 1 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "fps: 11.172446492548247\n",
            "TIMESTEP 2 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "fps: 10.768044280821742\n",
            "TIMESTEP 3 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "fps: 9.616654820589247\n",
            "TIMESTEP 4 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "fps: 15.424828717375394\n",
            "TIMESTEP 5 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "fps: 11.103797361119936\n",
            "TIMESTEP 6 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "fps: 10.440916264643358\n",
            "TIMESTEP 7 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "fps: 10.596198882856573\n",
            "TIMESTEP 8 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "fps: 10.888922350009086\n",
            "TIMESTEP 9 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "fps: 32.00879147715132\n",
            "TIMESTEP 10 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "fps: 7.954544420084812\n",
            "TIMESTEP 11 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "fps: 10.55383221780484\n",
            "TIMESTEP 12 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "fps: 9.252760294990326\n",
            "TIMESTEP 13 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "fps: 32.18415923635303\n",
            "TIMESTEP 14 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "fps: 10.261595447450444\n",
            "TIMESTEP 15 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "fps: 11.012082482238593\n",
            "TIMESTEP 16 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "fps: 9.16437209810455\n",
            "TIMESTEP 17 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "fps: 9.388460238477363\n",
            "TIMESTEP 18 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "fps: 31.754822688592107\n",
            "TIMESTEP 19 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "fps: 10.230608620992449\n",
            "TIMESTEP 20 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "fps: 9.352077754415381\n",
            "TIMESTEP 21 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "fps: 10.49607239112428\n",
            "TIMESTEP 22 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "fps: 10.175804980300061\n",
            "TIMESTEP 23 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "fps: 11.289486788166547\n",
            "TIMESTEP 24 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "fps: 7.890288913386183\n",
            "TIMESTEP 25 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "fps: 9.367701416667225\n",
            "TIMESTEP 26 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "1/1 [==============================] - 0s 77ms/step\n",
            "fps: 5.828312093635317\n",
            "TIMESTEP 27 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "fps: 9.26823792498442\n",
            "TIMESTEP 28 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "fps: 8.460266456889856\n",
            "TIMESTEP 29 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "fps: 7.740953393388738\n",
            "TIMESTEP 30 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "fps: 7.565892815267781\n",
            "TIMESTEP 31 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "fps: 9.59307628618871\n",
            "TIMESTEP 32 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "fps: 10.7796122293727\n",
            "TIMESTEP 33 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "fps: 9.93131927327144\n",
            "TIMESTEP 34 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "fps: 8.425071710362449\n",
            "TIMESTEP 35 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "fps: 4.633868093479358\n",
            "TIMESTEP 36 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "fps: 7.318318548865516\n",
            "TIMESTEP 37 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "fps: 2.796275369826282\n",
            "TIMESTEP 38 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "fps: 6.518593117896546\n",
            "TIMESTEP 39 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "fps: 8.794786415828629\n",
            "TIMESTEP 40 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "fps: 7.213016194575668\n",
            "TIMESTEP 41 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "fps: 5.8244029517056735\n",
            "TIMESTEP 42 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "fps: 6.421605897528152\n",
            "TIMESTEP 43 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "fps: 5.92449290920391\n",
            "TIMESTEP 44 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "fps: 5.031307241366488\n",
            "TIMESTEP 45 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "fps: 16.710174778787504\n",
            "TIMESTEP 46 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "fps: 8.020805923568828\n",
            "TIMESTEP 47 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "fps: 6.661474248613485\n",
            "TIMESTEP 48 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD -1 / Q_MAX  0 / Loss  0\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "fps: 6.820483383391034\n",
            "TIMESTEP 49 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "fps: 11.265774029357651\n",
            "TIMESTEP 50 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "fps: 10.666069911859994\n",
            "TIMESTEP 51 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "fps: 16.709975060357124\n",
            "TIMESTEP 52 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "fps: 10.336169782818164\n",
            "TIMESTEP 53 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "fps: 15.914461227912412\n",
            "TIMESTEP 54 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "fps: 10.230708438625078\n",
            "TIMESTEP 55 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "fps: 11.265320154705629\n",
            "TIMESTEP 56 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "fps: 10.780083222173388\n",
            "TIMESTEP 57 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "fps: 11.140131313345941\n",
            "TIMESTEP 58 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "fps: 33.42021641089385\n",
            "TIMESTEP 59 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "fps: 7.872591141067584\n",
            "TIMESTEP 60 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "fps: 10.86643850699635\n",
            "TIMESTEP 61 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "fps: 10.721992295244986\n",
            "TIMESTEP 62 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "fps: 15.665820061553172\n",
            "TIMESTEP 63 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "fps: 10.897834361805781\n",
            "TIMESTEP 64 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "fps: 9.283233181722608\n",
            "TIMESTEP 65 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "fps: 8.643273407684001\n",
            "TIMESTEP 66 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "fps: 7.894640353106149\n",
            "TIMESTEP 67 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "fps: 10.78063738404715\n",
            "TIMESTEP 68 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "fps: 8.951820754596174\n",
            "TIMESTEP 69 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "fps: 11.658325203116444\n",
            "TIMESTEP 70 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "fps: 11.01763639706848\n",
            "TIMESTEP 71 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "fps: 8.068063824262069\n",
            "TIMESTEP 72 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "fps: 8.003480517497996\n",
            "TIMESTEP 73 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "fps: 9.324778458077109\n",
            "TIMESTEP 74 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "fps: 10.780803643719493\n",
            "TIMESTEP 75 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "fps: 10.443906046518578\n",
            "TIMESTEP 76 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "fps: 7.426714757976411\n",
            "TIMESTEP 77 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "fps: 7.924621416088045\n",
            "TIMESTEP 78 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "fps: 8.18330523818443\n",
            "TIMESTEP 79 / STATE observe / EPSILON 0.1 / ACTION 1 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "fps: 10.497096864613782\n",
            "TIMESTEP 80 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "fps: 10.780166342822483\n",
            "TIMESTEP 81 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "fps: 9.198620087681014\n",
            "TIMESTEP 82 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "fps: 8.794786415828629\n",
            "TIMESTEP 83 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "fps: 11.017723221439136\n",
            "TIMESTEP 84 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "fps: 10.443880041035447\n",
            "TIMESTEP 85 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "fps: 9.73425145225712\n",
            "TIMESTEP 86 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "fps: 9.279659460739088\n",
            "TIMESTEP 87 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "fps: 11.268891623365807\n",
            "TIMESTEP 88 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "fps: 9.42068132149507\n",
            "TIMESTEP 89 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "fps: 10.77698096821355\n",
            "TIMESTEP 90 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "fps: 10.406591852482242\n",
            "TIMESTEP 91 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "fps: 10.595797355537254\n",
            "TIMESTEP 92 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "fps: 10.83413149832876\n",
            "TIMESTEP 93 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "fps: 7.286686170545437\n",
            "TIMESTEP 94 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "fps: 10.613090553920664\n",
            "TIMESTEP 95 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "fps: 10.245177616617774\n",
            "TIMESTEP 96 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD -1 / Q_MAX  0 / Loss  0\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "fps: 11.07146835463086\n",
            "TIMESTEP 97 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "fps: 10.6136813950134\n",
            "TIMESTEP 98 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "fps: 10.88977048499325\n",
            "TIMESTEP 99 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "fps: 10.690319259021374\n",
            "TIMESTEP 100 / STATE observe / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "fps: 10.287494266744172\n",
            "TIMESTEP 101 / STATE explore / EPSILON 0.1 / ACTION 0 / REWARD 0.1 / Q_MAX  0 / Loss  0\n",
            "----------Random Action----------\n",
            "fps: 16.683454982995563\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 70ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "TIMESTEP 102 / STATE explore / EPSILON 0.099999001 / ACTION 1 / REWARD 0.1 / Q_MAX  2.0765448 / Loss  1.3157293796539307\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "fps: 0.33898063448793103\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "TIMESTEP 103 / STATE explore / EPSILON 0.099998002 / ACTION 0 / REWARD 0.1 / Q_MAX  4.0974383 / Loss  1.280806064605713\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "fps: 0.4251605868412706\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "TIMESTEP 104 / STATE explore / EPSILON 0.099997003 / ACTION 0 / REWARD -1 / Q_MAX  2.911304 / Loss  0.8290640115737915\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "fps: 0.4126565194364941\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 71ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "TIMESTEP 105 / STATE explore / EPSILON 0.099996004 / ACTION 0 / REWARD 0.1 / Q_MAX  3.3917935 / Loss  1.132089376449585\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "fps: 0.4098019395104267\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "TIMESTEP 106 / STATE explore / EPSILON 0.099995005 / ACTION 0 / REWARD -1 / Q_MAX  5.61092 / Loss  0.16671089828014374\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "fps: 0.41554390100270194\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 64ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "TIMESTEP 107 / STATE explore / EPSILON 0.099994006 / ACTION 1 / REWARD 0.1 / Q_MAX  2.7569013 / Loss  0.9119633436203003\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "fps: 0.41927769906200163\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "TIMESTEP 108 / STATE explore / EPSILON 0.099993007 / ACTION 0 / REWARD -1 / Q_MAX  3.3797154 / Loss  1.8757011890411377\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "fps: 0.21445480122544316\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "TIMESTEP 109 / STATE explore / EPSILON 0.099992008 / ACTION 0 / REWARD 0.1 / Q_MAX  2.8851292 / Loss  0.5738340020179749\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "fps: 0.430864111672212\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "TIMESTEP 110 / STATE explore / EPSILON 0.09999100899999999 / ACTION 1 / REWARD 0.1 / Q_MAX  2.6483858 / Loss  0.8398132920265198\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "fps: 0.4078347208659238\n",
            "1/1 [==============================] - 0s 69ms/step\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[23], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m playGame(observe\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m);\n",
            "Cell \u001b[1;32mIn[17], line 8\u001b[0m, in \u001b[0;36mplayGame\u001b[1;34m(observe)\u001b[0m\n\u001b[0;32m      6\u001b[0m model \u001b[39m=\u001b[39m buildmodel()\n\u001b[0;32m      7\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m----> 8\u001b[0m     trainNetwork(model,game_state,observe\u001b[39m=\u001b[39;49mobserve)\n\u001b[0;32m      9\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[0;32m     10\u001b[0m     game\u001b[39m.\u001b[39mend()\n",
            "Cell \u001b[1;32mIn[22], line 101\u001b[0m, in \u001b[0;36mtrainNetwork\u001b[1;34m(model, game_state, observe)\u001b[0m\n\u001b[0;32m     98\u001b[0m inputs[i:i \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m] \u001b[39m=\u001b[39m state_t    \n\u001b[0;32m    100\u001b[0m targets[i] \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(state_t)  \u001b[39m# predicted q values\u001b[39;00m\n\u001b[1;32m--> 101\u001b[0m Q_sa \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mpredict(state_t1)      \u001b[39m#predict q values for next step\u001b[39;00m\n\u001b[0;32m    103\u001b[0m \u001b[39mif\u001b[39;00m terminal:\n\u001b[0;32m    104\u001b[0m     targets[i, action_t] \u001b[39m=\u001b[39m reward_t \u001b[39m# if terminated, only equals reward\u001b[39;00m\n",
            "File \u001b[1;32md:\\DinoRunTutorial\\.venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
            "File \u001b[1;32md:\\DinoRunTutorial\\.venv\\lib\\site-packages\\keras\\engine\\training.py:2317\u001b[0m, in \u001b[0;36mModel.predict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   2308\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m:\n\u001b[0;32m   2309\u001b[0m         warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m   2310\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mUsing Model.predict with MultiWorkerMirroredStrategy \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2311\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mor TPUStrategy and AutoShardPolicy.FILE might lead to \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2314\u001b[0m             stacklevel\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m,\n\u001b[0;32m   2315\u001b[0m         )\n\u001b[1;32m-> 2317\u001b[0m data_handler \u001b[39m=\u001b[39m data_adapter\u001b[39m.\u001b[39;49mget_data_handler(\n\u001b[0;32m   2318\u001b[0m     x\u001b[39m=\u001b[39;49mx,\n\u001b[0;32m   2319\u001b[0m     batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[0;32m   2320\u001b[0m     steps_per_epoch\u001b[39m=\u001b[39;49msteps,\n\u001b[0;32m   2321\u001b[0m     initial_epoch\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m,\n\u001b[0;32m   2322\u001b[0m     epochs\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[0;32m   2323\u001b[0m     max_queue_size\u001b[39m=\u001b[39;49mmax_queue_size,\n\u001b[0;32m   2324\u001b[0m     workers\u001b[39m=\u001b[39;49mworkers,\n\u001b[0;32m   2325\u001b[0m     use_multiprocessing\u001b[39m=\u001b[39;49muse_multiprocessing,\n\u001b[0;32m   2326\u001b[0m     model\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[0;32m   2327\u001b[0m     steps_per_execution\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_steps_per_execution,\n\u001b[0;32m   2328\u001b[0m )\n\u001b[0;32m   2330\u001b[0m \u001b[39m# Container that configures and calls `tf.keras.Callback`s.\u001b[39;00m\n\u001b[0;32m   2331\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(callbacks, callbacks_module\u001b[39m.\u001b[39mCallbackList):\n",
            "File \u001b[1;32md:\\DinoRunTutorial\\.venv\\lib\\site-packages\\keras\\engine\\data_adapter.py:1579\u001b[0m, in \u001b[0;36mget_data_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1577\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(kwargs[\u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m], \u001b[39m\"\u001b[39m\u001b[39m_cluster_coordinator\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m):\n\u001b[0;32m   1578\u001b[0m     \u001b[39mreturn\u001b[39;00m _ClusterCoordinatorDataHandler(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m-> 1579\u001b[0m \u001b[39mreturn\u001b[39;00m DataHandler(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
            "File \u001b[1;32md:\\DinoRunTutorial\\.venv\\lib\\site-packages\\keras\\engine\\data_adapter.py:1259\u001b[0m, in \u001b[0;36mDataHandler.__init__\u001b[1;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute)\u001b[0m\n\u001b[0;32m   1256\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_steps_per_execution \u001b[39m=\u001b[39m steps_per_execution\n\u001b[0;32m   1258\u001b[0m adapter_cls \u001b[39m=\u001b[39m select_data_adapter(x, y)\n\u001b[1;32m-> 1259\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_adapter \u001b[39m=\u001b[39m adapter_cls(\n\u001b[0;32m   1260\u001b[0m     x,\n\u001b[0;32m   1261\u001b[0m     y,\n\u001b[0;32m   1262\u001b[0m     batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[0;32m   1263\u001b[0m     steps\u001b[39m=\u001b[39;49msteps_per_epoch,\n\u001b[0;32m   1264\u001b[0m     epochs\u001b[39m=\u001b[39;49mepochs \u001b[39m-\u001b[39;49m initial_epoch,\n\u001b[0;32m   1265\u001b[0m     sample_weights\u001b[39m=\u001b[39;49msample_weight,\n\u001b[0;32m   1266\u001b[0m     shuffle\u001b[39m=\u001b[39;49mshuffle,\n\u001b[0;32m   1267\u001b[0m     max_queue_size\u001b[39m=\u001b[39;49mmax_queue_size,\n\u001b[0;32m   1268\u001b[0m     workers\u001b[39m=\u001b[39;49mworkers,\n\u001b[0;32m   1269\u001b[0m     use_multiprocessing\u001b[39m=\u001b[39;49muse_multiprocessing,\n\u001b[0;32m   1270\u001b[0m     distribution_strategy\u001b[39m=\u001b[39;49mtf\u001b[39m.\u001b[39;49mdistribute\u001b[39m.\u001b[39;49mget_strategy(),\n\u001b[0;32m   1271\u001b[0m     model\u001b[39m=\u001b[39;49mmodel,\n\u001b[0;32m   1272\u001b[0m )\n\u001b[0;32m   1274\u001b[0m strategy \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mdistribute\u001b[39m.\u001b[39mget_strategy()\n\u001b[0;32m   1276\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_current_step \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
            "File \u001b[1;32md:\\DinoRunTutorial\\.venv\\lib\\site-packages\\keras\\engine\\data_adapter.py:347\u001b[0m, in \u001b[0;36mTensorLikeDataAdapter.__init__\u001b[1;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[0;32m    343\u001b[0m     \u001b[39mreturn\u001b[39;00m flat_dataset\n\u001b[0;32m    345\u001b[0m indices_dataset \u001b[39m=\u001b[39m indices_dataset\u001b[39m.\u001b[39mflat_map(slice_batch_indices)\n\u001b[1;32m--> 347\u001b[0m dataset \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mslice_inputs(indices_dataset, inputs)\n\u001b[0;32m    349\u001b[0m \u001b[39mif\u001b[39;00m shuffle \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbatch\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    351\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mshuffle_batch\u001b[39m(\u001b[39m*\u001b[39mbatch):\n",
            "File \u001b[1;32md:\\DinoRunTutorial\\.venv\\lib\\site-packages\\keras\\engine\\data_adapter.py:388\u001b[0m, in \u001b[0;36mTensorLikeDataAdapter.slice_inputs\u001b[1;34m(self, indices_dataset, inputs)\u001b[0m\n\u001b[0;32m    383\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgrab_batch\u001b[39m(i, data):\n\u001b[0;32m    384\u001b[0m     \u001b[39mreturn\u001b[39;00m tf\u001b[39m.\u001b[39mnest\u001b[39m.\u001b[39mmap_structure(\n\u001b[0;32m    385\u001b[0m         \u001b[39mlambda\u001b[39;00m d: tf\u001b[39m.\u001b[39mgather(d, i, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m), data\n\u001b[0;32m    386\u001b[0m     )\n\u001b[1;32m--> 388\u001b[0m dataset \u001b[39m=\u001b[39m dataset\u001b[39m.\u001b[39;49mmap(grab_batch, num_parallel_calls\u001b[39m=\u001b[39;49mtf\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mAUTOTUNE)\n\u001b[0;32m    390\u001b[0m \u001b[39m# Default optimizations are disabled to avoid the overhead of\u001b[39;00m\n\u001b[0;32m    391\u001b[0m \u001b[39m# (unnecessary) input pipeline graph serialization and deserialization\u001b[39;00m\n\u001b[0;32m    392\u001b[0m options \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mOptions()\n",
            "File \u001b[1;32md:\\DinoRunTutorial\\.venv\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:2296\u001b[0m, in \u001b[0;36mDatasetV2.map\u001b[1;34m(self, map_func, num_parallel_calls, deterministic, name)\u001b[0m\n\u001b[0;32m   2294\u001b[0m   \u001b[39mreturn\u001b[39;00m MapDataset(\u001b[39mself\u001b[39m, map_func, preserve_cardinality\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, name\u001b[39m=\u001b[39mname)\n\u001b[0;32m   2295\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 2296\u001b[0m   \u001b[39mreturn\u001b[39;00m ParallelMapDataset(\n\u001b[0;32m   2297\u001b[0m       \u001b[39mself\u001b[39;49m,\n\u001b[0;32m   2298\u001b[0m       map_func,\n\u001b[0;32m   2299\u001b[0m       num_parallel_calls,\n\u001b[0;32m   2300\u001b[0m       deterministic,\n\u001b[0;32m   2301\u001b[0m       preserve_cardinality\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m   2302\u001b[0m       name\u001b[39m=\u001b[39;49mname)\n",
            "File \u001b[1;32md:\\DinoRunTutorial\\.venv\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:5555\u001b[0m, in \u001b[0;36mParallelMapDataset.__init__\u001b[1;34m(self, input_dataset, map_func, num_parallel_calls, deterministic, use_inter_op_parallelism, preserve_cardinality, use_legacy_function, name)\u001b[0m\n\u001b[0;32m   5552\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_parallel_calls \u001b[39m=\u001b[39m ops\u001b[39m.\u001b[39mconvert_to_tensor(\n\u001b[0;32m   5553\u001b[0m     num_parallel_calls, dtype\u001b[39m=\u001b[39mdtypes\u001b[39m.\u001b[39mint64, name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mnum_parallel_calls\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   5554\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_name \u001b[39m=\u001b[39m name\n\u001b[1;32m-> 5555\u001b[0m variant_tensor \u001b[39m=\u001b[39m gen_dataset_ops\u001b[39m.\u001b[39mparallel_map_dataset_v2(\n\u001b[0;32m   5556\u001b[0m     input_dataset\u001b[39m.\u001b[39m_variant_tensor,  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   5557\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_map_func\u001b[39m.\u001b[39mfunction\u001b[39m.\u001b[39mcaptured_inputs,\n\u001b[0;32m   5558\u001b[0m     f\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_map_func\u001b[39m.\u001b[39mfunction,\n\u001b[0;32m   5559\u001b[0m     num_parallel_calls\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_parallel_calls,\n\u001b[0;32m   5560\u001b[0m     deterministic\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_deterministic,\n\u001b[0;32m   5561\u001b[0m     use_inter_op_parallelism\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_use_inter_op_parallelism,\n\u001b[0;32m   5562\u001b[0m     preserve_cardinality\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_preserve_cardinality,\n\u001b[0;32m   5563\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_common_args)\n\u001b[0;32m   5564\u001b[0m \u001b[39msuper\u001b[39m(ParallelMapDataset, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(input_dataset, variant_tensor)\n",
            "File \u001b[1;32md:\\DinoRunTutorial\\.venv\\lib\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py:6184\u001b[0m, in \u001b[0;36mparallel_map_dataset_v2\u001b[1;34m(input_dataset, other_arguments, num_parallel_calls, f, output_types, output_shapes, use_inter_op_parallelism, deterministic, preserve_cardinality, metadata, name)\u001b[0m\n\u001b[0;32m   6182\u001b[0m \u001b[39mif\u001b[39;00m tld\u001b[39m.\u001b[39mis_eager:\n\u001b[0;32m   6183\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 6184\u001b[0m     _result \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_FastPathExecute(\n\u001b[0;32m   6185\u001b[0m       _ctx, \u001b[39m\"\u001b[39;49m\u001b[39mParallelMapDatasetV2\u001b[39;49m\u001b[39m\"\u001b[39;49m, name, input_dataset, other_arguments,\n\u001b[0;32m   6186\u001b[0m       num_parallel_calls, \u001b[39m\"\u001b[39;49m\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m, f, \u001b[39m\"\u001b[39;49m\u001b[39moutput_types\u001b[39;49m\u001b[39m\"\u001b[39;49m, output_types,\n\u001b[0;32m   6187\u001b[0m       \u001b[39m\"\u001b[39;49m\u001b[39moutput_shapes\u001b[39;49m\u001b[39m\"\u001b[39;49m, output_shapes, \u001b[39m\"\u001b[39;49m\u001b[39muse_inter_op_parallelism\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m   6188\u001b[0m       use_inter_op_parallelism, \u001b[39m\"\u001b[39;49m\u001b[39mdeterministic\u001b[39;49m\u001b[39m\"\u001b[39;49m, deterministic,\n\u001b[0;32m   6189\u001b[0m       \u001b[39m\"\u001b[39;49m\u001b[39mpreserve_cardinality\u001b[39;49m\u001b[39m\"\u001b[39;49m, preserve_cardinality, \u001b[39m\"\u001b[39;49m\u001b[39mmetadata\u001b[39;49m\u001b[39m\"\u001b[39;49m, metadata)\n\u001b[0;32m   6190\u001b[0m     \u001b[39mreturn\u001b[39;00m _result\n\u001b[0;32m   6191\u001b[0m   \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "playGame(observe=False);"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "060aa41733d01bc28f5dcb03e30a06471eeea87bd2c2813165748daf9d9436ac"
      }
    },
    "colab": {
      "provenance": [],
      "history_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}